---
title: "BCB420 A1 - Dataset selection and initial processing"
author: "Yijia Chen"
date: "2022-02-15"
output: html_notebook
---

## 1. Setting up

Several R packages are required for this assignment. They can be installed by running the following snippet (if not
already installed):
```{r}
if (!requireNamespace("BiocManager", quietly = TRUE)) {
  install.packages("BiocManager")
}
if (!requireNamespace("GEOquery", quietly = TRUE)) {
  BioCManager::install("GEOquery")
}
if (!requireNamespace("knitr", quietly = TRUE)) {
  install.packages("knitr")
}
if (!requireNamespace("edgeR", quietly = TRUE)) {
  BioCManager::install("edgeR")
}
if (!requireNamespace("biomaRt", quietly = TRUE)) {
  BiocManager::install("biomaRt")
}
if (!requireNamespace("plyr", quietly = TRUE)) {
  install.package("plyr")
}
if (!requireNamespace("limma", quietly = TRUE)) {
  BioCManager::install("limma")
}
```

## 2. Selecting a dataset

The process of selecting a dataset for this assignment has been documented in greater detail as a
[journal entry](https://github.com/bcb420-2022/Yijia_Chen/wiki/5.-Assignment-1-%E2%80%90--Selecting-a-dataset) in my
wiki. In summary, I

- Visited the [GEO website](https://www.ncbi.nlm.nih.gov/geo/) and selected "Series" under "Browse Content"
- Filtered the results by "Organism(s): Homo sapiens" and "Series type(s): Expression profiling by high throughput
sequencing"
- Sorted the results by number of samples, descending
- Advanced through the list and randomly opened datasets
- Went through each opened dataset and closed inadequate ones (e.g. those missing a publication or counts data)
- Chose an interesting experiment from the remaining suitable datasets

The final chosen dataset was [GSE157194](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE157194), "Atopic
dermatitis displays stable and dynamic skin transcriptome signatures".

Below are some quick facts about the dataset.

```{r}
# Code adapted from lecture 4
gse <- GEOquery::getGEO("GSE157194", GSEMatrix = FALSE)
gpl <- names(GEOquery::GPLList(gse))[1]
gplInfo <- GEOquery::Meta(GEOquery::getGEO(gpl))
```

Platform title: `r gplInfo$title`  
Submission date: `r gplInfo$submission_date`  
Last update: `r gplInfo$last_update_date`  
Organism: `r gplInfo$organism`  

## 3. Downloading the data

The dataset's raw gene counts matrix (a gzipped text file) is the only supplementary file provided. We can download it
using the following snippet, first checking to see if it has already been downloaded:
```{r}
# The supplementary file will be downloaded to a subdirectory in the current working directory
if (!dir.exists("GSE157194")) {
  suppFiles <- GEOquery::getGEOSuppFiles("GSE157194")
  fileName <- rownames(suppFiles)
  # The gz file will be replaced by a txt file of the same name
  GEOquery::gunzip(fileName)
}

# Set the relative filename (without the gz extension)
fileName <- "GSE157194/GSE157194_Raw_gene_counts_matrix.txt"
# Read the matrix (txt/tsv file)
countsMatrix <- read.table(fileName, header = TRUE, check.names = FALSE)
```

We can take a look at the first few rows and columns of the matrix to check that the data has been read in properly:
```{r}
knitr::kable(countsMatrix[1:4, 1:5], format = "html")
```

## 4. Cleaning the data

We start by examining the data; for example, by checking the dimensions of the matrix and counting the number of unique
genes:

Number of rows (genes, possibly duplicate): `r dim(countsMatrix)[1]`  
Number of columns (samples): `r dim(countsMatrix)[2] - 1  # Subtract 1 to account for the gene name`

Since the entries in the "Gene" column take the form of "ENSG00000000000" (with the zeroes being different numbers), we
can assume that the genes are represented by their Ensembl gene IDs, and so there should be no rows with duplicate IDs.

Number of gene IDs: `r length(countsMatrix$Gene)`  
Number of unique gene IDs: `r length(unique(countsMatrix$Gene))`

We see that the number of gene IDs is equal to the number of unique gene IDs, so no duplicates need to be taken care
of. The next step is to filter out genes that have low counts; i.e., those that are weakly expressed and
noninformative:
```{r}
# Compute the counts per million for the matrix
countsPerMil <- edgeR::cpm(countsMatrix[, 2:dim(countsMatrix)[2]])
rownames(countsPerMil) <- countsMatrix[, 1]

# The smallest size of a group is just a single sample
rowsToKeep <- rowSums(countsPerMil > 1) >= 1
filteredCounts <- countsMatrix[rowsToKeep, ]
rownames(filteredCounts) <- filteredCounts$Gene
```

Number of rows before filtering: `r length(countsMatrix$Gene)`  
Number of rows after filtering: `r length(filteredCounts$Gene)`  
Change in coverage: `r ((length(filteredCounts$Gene) - length(countsMatrix$Gene)) / length(countsMatrix$Gene)) * 100`%

## 5. Mapping rows to HUGO symbols

The counts matrix in its current form has its rows labelled by Ensembl gene IDs. In order to have the rows labelled by
HUGO gene symbol instead, we can use the package "biomaRt" to convert between the two.
```{r}
# Do a batch conversion on the entire first column of the counts matrix, creating a mapping between Ensembl gene IDs
# and HUGO gene symbols
# httr::set_config(httr::config(ssl_verifypeer = FALSE))
mart <- biomaRt::useEnsembl(biomart = "ensembl", dataset = "hsapiens_gene_ensembl")
hugoSymbols <- biomaRt::getBM(attributes = c("ensembl_gene_id", "hgnc_symbol"),
                              filters = "ensembl_gene_id",
                              values = filteredCounts$Gene,
                              mart = mart)
```

After the operation completes (it may take some time), it is necessary to check that all conversions were successful.
In particular, a key indicator to look out for is the presence of empty strings post-conversion. This can be checked
with the following:
```{r}
# Can probably optimize this with a vector operation
numEmpty <- 0
for (i in 1:length(hugoSymbols$hgnc_symbol)) {
  # Found a failed conversion (mapping from Ensembl ID to empty string)
  if (hugoSymbols$hgnc_symbol[i] == "") {
    numEmpty <- numEmpty + 1
  }
}
```

Number of failed mappings: `r numEmpty`

To examine these failures in more detail, we can take a look at one the of the Ensembl gene IDs that was unable to be
mapped to a HUGO symbol; for example, "ENSG00000280434". When we search for this ID in Ensembl's human database, we
find that this is a
[novel gene](https://useast.ensembl.org/Homo_sapiens/Gene/Summary?db=core;g=ENSG00000280434;r=22:44139365-44153626;t=ENST00000624919),
so it is not surprising that it does not have a direct mapping.

At this point, I was unsure what to do with these unsuccessful conversions. I considered just removing their rows, but
thought that might not be appropriate, as all of these genes have already survived filtering, and so cannot entirely be
called insignificant. Thus, for these particular genes, I decided to keep their Ensembl IDs in place of HUGO symbols,
with the understanding that they could be easily removed in the future if need be.

```{r}
# Can probably optimize this with a vector operation
for (i in 1:length(hugoSymbols$hgnc_symbol)) {
  if (hugoSymbols$hgnc_symbol[i] == "") {
    # Fill in the empty HUGO symbol with the Ensembl ID
    hugoSymbols$hgnc_symbol[i] <- hugoSymbols$ensembl_gene_id[i]
  }
}
```

A second check that should be done is to make sure that all rows have unique HUGO symbols; if multiple rows have the
same symbol, it means that multiple Ensembl IDs matched to a single HUGO symbol.

Number of symbols: `r length(hugoSymbols$hgnc_symbol)`  
Number of unique symbols: `r unique(length(hugoSymbols$hgnc_symbol))`

We see that the number of symbols is equal to the number of unique symbols, so there is nothing more to deal with.
The last step is to simply add the symbols to the counts matrix:
```{r}
# Rename the Ensembl column of the HUGO map so that it can be merged into the counts matrix
names(hugoSymbols)[names(hugoSymbols) == "ensembl_gene_id"] <- "Gene"
filteredCounts <- merge(filteredCounts, hugoSymbols, by = "Gene", all = TRUE)

# biomaRt misses some IDs, so we need to fill them in manually
for (i in 1:length(filteredCounts$Gene)) {
  if (is.na(filteredCounts$hgnc_symbol[i])) {
    filteredCounts$hgnc_symbol[i] <- filteredCounts$Gene[i]
  }
}

# Combine any rows with duplicate symbols
filteredCounts <- plyr::ddply(filteredCounts, "hgnc_symbol", plyr::numcolwise(sum))

# For some reason `rownames(filteredCounts) <- filteredCounts$hgnc_symbol` doesn't work
for (i in 1:length(filteredCounts$hgnc_symbol)) {
  rownames(filteredCounts)[i] <- filteredCounts$hgnc_symbol[i]
}

# Remove the "hgnc_symbol" column since it's now the row names
numCols <- length(filteredCounts[, 1])
filteredCounts <- filteredCounts[, -numCols]
```

## 6. Normalizing the data

Before we start normalizing the data, we should try to visualize it in order to have a point of reference to the
post-normalized results. Without these, we may accidentally normalize incorrectly. We can start with a simple box plot.

Since there are many samples in the dataset, we can get a good look at what the "overall picture" looks like, but it
also makes specific details harder to see. Thus, we can add the ability to just look particular subsets of the data by
introducing some mutable parameters directly in the code snippet.
```{r}
preNormBoxPlot <- log2(edgeR::cpm(filteredCounts[, 2:length(filteredCounts[1, ])]))
# Parameters to control the subset of data to visualize. The default values here correspond to the entire dataset
useRows <- 1:length(filteredCounts[, 1])
useCols <- 1:length(filteredCounts[1, ]) - 1
boxplot(preNormBoxPlot[useRows, useCols],
        xlab = "Samples",
        ylab = "log2 CPM",
        las = 2,
        cex = 0.3,
        cex.lab = 0.6,
        cex.axis = 0.3,
        main = "Pre-normalized CPM vs samples")
```

Looking at the edges of a density plot may also help in normalizing the data:
```{r}
preNormDensPlot <- apply(log2(edgeR::cpm(filteredCounts[, 2:length(filteredCounts[1, ])])), 2, density)

# Find the limits over the entire dataset
xlim <- 0
ylim <- 0
for (i in 1:length(preNormDensPlot)) {
  xlim <- range(c(xlim, preNormDensPlot[[i]]$x))
  ylim <- range(c(ylim, preNormDensPlot[[i]]$y))
}

cols <- rainbow(length(preNormDensPlot))
ltys <- rep(1, length(preNormDensPlot))

plot(preNormDensPlot[[1]],
     xlim = xlim,
     ylim = ylim,
     type = "n",
     main = "Pre-normalized counts density",
     cex.lab = 0.85)
for (i in 1:length(preNormDensPlot)) {
  lines(preNormDensPlot[[i]], col = cols[i], lty = ltys[i])
}
# Note that when using the entire dataset, not all the legends can fit in the plot
legend("topright",
       colnames(preNormBoxPlot),
       col = cols,
       lty = ltys,
       cex = 0.5,
       border = "blue",
       text.col = "green4",
       merge = TRUE,
       bg = "gray90")
```

Before we can actually normalize the data, we need to determine by what groups the data should be normalized to. Here,
we split the sample names into parts; e.g. "Patient_15_AL_m0" becomes an entry with "patient = 15", "sample.type = AL",
and "sample.time = m0".
```{r}
# Having the splitting function separate (i.e. not a lambda) makes the code a bit cleaner
splitColNames <- function(colNames) {
  return(unlist(strsplit(colNames, split = "_"))[c(2, 3, 4)])
}

colGroups <- data.frame(lapply(colnames(filteredCounts)[2:length(filteredCounts[1, ])], splitColNames))
colnames(colGroups) <- colnames(filteredCounts)[2:length(filteredCounts[1, ])]
rownames(colGroups) <- c("patient", "sample.type", "sample.time")
# Flip the table since we want rows to correspond to samples
colGroups <- data.frame(t(colGroups))
```

Now that we have all the prerequisite information and data structures set up, we can go ahead with normalizing the
counts matrix. The method of choice was trimmed mean of m-values (TMM), because the data fell into the assumption that
most genes are not differentially expressed. Additionally, the results were similar to normalization by relative log
expression (RLE).
```{r}
# We need the counts matrix to actually be a matrix
filteredCountsMatrix <- as.matrix(filteredCounts[, 2:length(filteredCounts[1, ])])
rownames(filteredCountsMatrix) <- filteredCounts$hgnc_symbol
# Use edgeR to do the TMM process
normFactors <- edgeR::DGEList(counts = filteredCountsMatrix, group = colGroups$sample.time)
normFactors <- edgeR::calcNormFactors(normFactors)
normCounts <- edgeR::cpm(normFactors)
```

The first comparison we can do is to take a look at how the normalized box plot looks (which is evidently "cleaner"
than the pre-normalization plot):
```{r}
postNormBoxPlot <- log2(normCounts)
# Again, parameters. Use these to specify subsets
useRows <- 1:length(filteredCounts[, 1])
useCols <- 1:length(filteredCounts[1, ]) - 1
boxplot(postNormBoxPlot[useRows, useCols],
        xlab = "Samples",
        ylab = "log2 CPM",
        las = 2,
        cex = 0.3,
        cex.lab = 0.6,
        cex.axis = 0.3,
        main = "Post-normalized CPM vs samples")
```

Next, we check what the normalized density plot looks like (which appears to be slighly better at the edges):
```{r}
postNormDensPlot <- apply(log2(normCounts), 2, density)

xlim <- 0
ylim <- 0
for (i in 1:length(postNormDensPlot)) {
  xlim <- range(c(xlim, postNormDensPlot[[i]]$x))
  ylim <- range(c(ylim, postNormDensPlot[[i]]$y))
}

cols <- rainbow(length(postNormDensPlot))
ltys <- rep(1, length(postNormDensPlot))

plot(postNormDensPlot[[1]],
     xlim = xlim,
     ylim = ylim,
     type = "n",
     main = "Post-normalized counts density",
     cex.lab = 0.85)
for (i in 1:length(postNormDensPlot)) {
  lines(postNormDensPlot[[i]], col = cols[i], lty = ltys[i])
}
legend("topright",
       colnames(postNormBoxPlot),
       col = cols,
       lty = ltys,
       cex = 0.5,
       border = "blue",
       text.col = "green4",
       merge = TRUE,
       bg = "gray90")
```

We can also take a look at the normalized counts in a multidimensional scaling plot to visualize the distance between
samples. Again, the number of samples is quite large, so smaller ranges should be chosen to compare instead of the
entire counts matrix. By default we use the entire normalized counts matrix, so the initial plot is essentially
undreadable and useless.
```{r}
limma::plotMDS(normFactors, labels = rownames(colGroups), col = c("darkgreen", "blue")[factor(colGroups$sample.time)])
```

As a final step, we can convert the normalized counts matrix into a dataframe with 19,425 rows (unique genes) and 166
columns (samples), as required for this assignment:
```{r}
cleanedCounts <- as.data.frame(normCounts)
# Just take a quick peek at the data to make sure the rows and columns are labelled
knitr::kable(cleanedCounts[1:4, 1:5], format = "html")
```

## 7. Interpreting the data

The dataset supposedly contains information for 59 affected patients (30 of which underwent treatment), and 31 healthy
controls. However, the authors did not label which samples corresponded to affected vs. healthy patients, and this is
also not mentioned in the publication. I suppose the distinction will become more clear after conducting the expression
analysis. "AL" means that the sample was taken from a lesional area of the patient and "AN" means it was taken from a
non-lesional area. "m0" means the sample was taken before treatment started and "m3" means it was taken three months
into treatment. Of those treated, 22 patients were treated with dupilumab and 8 were treated with cyclosporine, but
unfortunately again, these are not labelled or distinguished between by the authors in the dataset.

This dataset was of interest to me, because atopic dermatitis is a quite common, possibly autoimmune, disease in the
general population. In addition, when I first found the dataset when browsing the GEO website, it appeared to be quite
appropriate for this assignment, with many "samples" available.

In the raw counts matrix, there were no expression values that were not unique to specific genes, but I suppose if
there were any, I would have just combined the rows with identical symbols together, taking the sum of their counts per
sample. However, there were expression values that could not be mapped to HUGO symbols (using biomaRt's conversion),
which was handled by simply setting the Ensembl gene ID as the HUGO symbol. I suppose another method to handle this
would be to just remove those rows entirely, but I was unsure as to whether that would destroy too much data.

No outliers were removed from the dataset, because after visualization, it did not seem that there were any values that
significantly differed from the majority of data points. Replicates were handled as individual samples, and after
filtering, 19,425 genes (or about 50-60%) of the original expression values remain.

## 8. References

Möbus, L., Rodriguez, E., Harder, I., Stölzl, D., Boraczynski, N., Gerdes, S., Kleinheinz, A., Abraham, S.,
Heratizadeh, A., Handrick, C., Haufe, E., Werfel, T., Schmitt, J., Weidinger, S., & the TREATgermany study group.
(2020). Atopic dermatitis displays stable and dynamic skin transcriptome signatures. *J Allergy Clin Immun, 147*(1),
213-223. https://doi.org/10.1016/j.jaci.2020.06.012.
